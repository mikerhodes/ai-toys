"""
codeexplorer.py

Implements an agent-model to allow LLM to explore a codebase
using tools, rather than trying to pre-create a large context
from the codebase ourselves.
"""

import argparse
import copy
import json
import logging
import os
import subprocess
from pathlib import Path
from typing import Any, Dict, Tuple, cast

import anthropic
import ollama
import ibm_watsonx_ai as wai
import ibm_watsonx_ai.foundation_models as waifm
from ibm_watsonx_ai.wml_client_error import WMLClientError
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Prompt


class OllamaAdapter:
    def __init__(self):
        self.ollama_client = ollama.Client()

    def prepare_messages(self, chat_history):
        messages = [
            {"role": "user", "content": prompt},
        ] + chat_history
        return messages

    def chat(self, **kwargs):
        # message = ollama_client.chat(
        #     model=chat_model,
        #     messages=messages,
        #     tools=openai_tools,
        #     options=ollama.Options(
        #         num_ctx=16384,
        #     ),
        # )
        kwargs["model"] = kwargs["model"] or "qwen3:8b"
        chat_response = self.ollama_client.chat(
            options=ollama.Options(
                num_ctx=16384,
            ),
            **kwargs,
        )
        logger.debug("\nResponse:")
        logger.debug(f"Stop Reason: {chat_response['done_reason']}")
        logger.debug(f"Content: {chat_response['message']}")
        return chat_response

    def get_response_text(self, chat_response) -> str:
        return chat_response.message.content

    def tools_for_model(self, openai_tools):
        return openai_tools

    def has_tool_use(self, chat_response):
        return bool(chat_response.message.tool_calls)

    def get_tool_use(self, chat_response) -> Tuple[str, Dict[str, str], str]:
        tool_call = chat_response["message"]["tool_calls"][0]
        f = tool_call["function"]
        tool_name = f["name"]
        tool_input = f["arguments"]
        return tool_name, tool_input, ""

    def format_assistant_history_message(self, chat_response):
        return {
            "role": "assistant",
            "content": chat_response.message.content,
        }

    def format_tool_result_message(
        self, tool_name: str, tool_use_id: str, tool_result: str
    ) -> Dict:
        return {
            "role": "tool",
            "content": tool_result,
            "name": tool_name,
        }


class AnthropicAdapter:
    def __init__(self):
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error("Set ANTHROPIC_API_KEY")
            raise ValueError("Set ANTHROPIC_API_KEY")
        self.client = anthropic.Anthropic(api_key=api_key)

    def prepare_messages(self, chat_history):
        # Cache to last prompt to speed up future inference
        cache_prompt = None
        if chat_history:
            cache_prompt = copy.deepcopy(chat_history[-1])
            cache_prompt["content"][0]["cache_control"] = {
                "type": "ephemeral"
            }
        messages = (
            [
                {"role": "user", "content": prompt},
            ]
            + chat_history[:-1]
            + ([cache_prompt] if cache_prompt else [])
        )
        return messages

    def chat(self, **kwargs):
        # message = self.client.messages.create(
        #     model=chat_model,
        #     max_tokens=8192,
        #     messages=cast(Iterable[MessageParam], messages),
        #     tools=cast(Iterable[ToolUnionParam], tools),
        # )
        kwargs["model"] = kwargs["model"] or "claude-3-7-sonnet-latest"
        chat_response = self.client.messages.create(
            max_tokens=8192, **kwargs
        )
        logger.debug("\nResponse:")
        logger.debug(f"Stop Reason: {chat_response.stop_reason}")
        logger.debug(f"Content: {chat_response.content}")
        logger.debug(
            "cache_creation_input_tokens: %d, cache_read_input_tokens: %d, input_tokens: %d",
            chat_response.usage.cache_creation_input_tokens,
            chat_response.usage.cache_read_input_tokens,
            chat_response.usage.input_tokens,
        )
        return chat_response

    def tools_for_model(self, openai_tools):
        """Convert an OpenAI function tools to Anthropic's tool format."""

        def convert_openai_tool_to_anthropic(openai_tool):
            function = openai_tool.get("function", {})

            return {
                "name": function.get("name", ""),
                "description": function.get("description", ""),
                "input_schema": {
                    "type": "object",
                    "properties": function.get("parameters", {}).get(
                        "properties", {}
                    ),
                    "required": function.get("parameters", {}).get(
                        "required", []
                    ),
                },
            }

        return [convert_openai_tool_to_anthropic(x) for x in openai_tools]

    def has_tool_use(self, message):
        return message.stop_reason == "tool_use"

    def get_response_text(self, message) -> str:
        text_block = None
        try:
            text_block = next(
                block for block in message.content if block.type == "text"
            )
        except StopIteration:
            pass  # sometimes the model has nothing to say
        return text_block.text if text_block else ""

    def get_tool_use(self, message) -> Tuple[str, Dict[str, str], str]:
        tool_use = next(
            block for block in message.content if block.type == "tool_use"
        )
        tool_name = tool_use.name
        tool_input = cast(Dict[str, str], tool_use.input)
        return tool_name, tool_input, tool_use.id

    def format_assistant_history_message(self, message):
        return {"role": "assistant", "content": message.content}

    def format_tool_result_message(
        self, tool_name: str, tool_use_id: str, tool_result: str
    ) -> Any:
        return {
            "role": "user",
            "content": [
                {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": tool_result,
                }
            ],
        }


class WatsonxAdapter:
    def __init__(self):
        self.ollama_client = ollama.Client()
        if v := os.environ.get("WATSONX_IAM_API_KEY"):
            wxapikey = v
        else:
            logger.error("WATSONX_IAM_API_KEY")
            raise ValueError("WATSONX_IAM_API_KEY")

        if v := os.environ.get("WATSONX_PROJECT"):
            self.wxproject_id = v
        else:
            logger.error("WATSONX_PROJECT")
            raise ValueError("WATSONX_PROJECT")

        if v := os.environ.get("WATSONX_URL"):
            wxendpoint = v
        else:
            logger.error("WATSONX_URL")
            raise ValueError("WATSONX_URL")

        credentials = wai.Credentials(
            url=wxendpoint,
            api_key=wxapikey,
        )
        self.wxclient = wai.APIClient(credentials)

    def prepare_messages(self, chat_history):
        messages = [
            {"role": "user", "content": prompt},
        ] + chat_history
        return messages

    def chat(self, **kwargs):
        kwargs["model"] = (
            kwargs["model"] or "meta-llama/llama-3-3-70b-instruct"
        )
        params = {
            "time_limit": 30000,
            "max_tokens": 8192,
        }
        wxmodel = waifm.ModelInference(
            model_id=kwargs["model"],
            api_client=self.wxclient,
            params=params,
            project_id=self.wxproject_id,
            space_id=None,
            verify=True,
        )
        chat_response = wxmodel.chat(
            messages=kwargs["messages"],
            tools=kwargs["tools"],
        )
        # logger.debug("\nResponse:")
        # logger.debug(f"Stop Reason: {chat_response['done_reason']}")
        # logger.debug(f"Content: {chat_response['message']}")
        return chat_response

    def get_response_text(self, chat_response) -> str:
        try:
            return chat_response["choices"][0]["message"]["content"]
        except KeyError:
            return ""

    def tools_for_model(self, openai_tools):
        return openai_tools

    def has_tool_use(self, chat_response) -> bool:
        try:
            return (
                len(chat_response["choices"][0]["message"]["tool_calls"]) > 0
            )
        except KeyError:
            return False

    def get_tool_use(self, chat_response) -> Tuple[str, Dict[str, str], str]:
        tool_call = chat_response["choices"][0]["message"]["tool_calls"][0]
        f = tool_call["function"]
        tool_name = f["name"]
        tool_input = json.loads(f["arguments"])
        return tool_name, tool_input, tool_call["id"]

    def format_assistant_history_message(self, chat_response):
        return {
            "role": "assistant",
            "content": self.get_response_text(chat_response),
        }

    def format_tool_result_message(
        self, tool_name: str, tool_use_id: str, tool_result: str
    ) -> Dict:
        return {
            "role": "tool",
            "tool_call_id": tool_use_id,
            "content": tool_result,
        }


logging.basicConfig(level=logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

parser = argparse.ArgumentParser(description="Code explorer tool")
parser.add_argument(
    "-n",
    "--num-turns",
    type=int,
    default=20,
    help="Number of turns (default: 20)",
)
parser.add_argument(
    "-p",
    "--provider",
    type=str,
    choices=["ollama", "anthropic", "watsonx"],
    required=True,
    help="Model provider",
)
parser.add_argument(
    "-m",
    "--model",
    type=str,
    help="Model (default: provider specific)",
)
parser.add_argument(
    "-e",
    "--allow-edits",
    action="store_true",
    help="Allow model to create and edit files (default: false)",
)
parser.add_argument(
    "-t",
    "--task",
    type=str,
    default=None,
    help="Task to complete using codebase (default: prompt user for question)",
)
parser.add_argument(
    "-o",
    "--output",
    type=str,
    default=None,
    help="Write final output to file (default: write only to terminal)",
)
parser.add_argument(
    "path",
    nargs="?",
    default=".",
    help="Path to explore (default: current directory)",
)
args = parser.parse_args()

client: OllamaAdapter | AnthropicAdapter | WatsonxAdapter
try:
    if args.provider == "ollama":
        client = OllamaAdapter()
    elif args.provider == "anthropic":
        client = AnthropicAdapter()
    elif args.provider == "watsonx":
        client = WatsonxAdapter()
    else:
        raise ValueError("Invalid model provider")
except Exception:
    logger.error("Cannot load provider")
    exit(1)

max_turns = args.num_turns
chat_model = args.model

# Let model expore this folder for now
pwd = Path(args.path).resolve()
jail = pwd

logger.info(
    "Config: turns: %d, path: %s, model: %s", max_turns, jail, chat_model
)

#
# Helpers
#


def is_git_working_copy_clean(directory: Path) -> bool:
    """
    Returns True if working copy is clean or not a git repo, False otherwise
    """
    try:
        status = subprocess.run(
            ["git", "-C", str(directory), "status", "--porcelain"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=False,
        )

        if status.returncode == 128:
            # Exit code 128 means "not a git repository" or
            # non-existent directory
            return True

        # Empty output means working copy is clean
        return status.returncode == 0 and not status.stdout.strip()

    except Exception as e:
        print(f"Error checking git status: {e}")
        return False


#
# Tools
#


def read_file_path(path: str) -> str:
    p = Path(path).absolute()
    if jail not in p.parents:
        return f"ERROR: Path {p} must have {jail} as an ancestor"
    if not p.exists():
        return f"ERROR: Path {path} does not exist"
    if not p.is_file():
        return f"ERROR: Path {path} is not a file"
    with open(path, "r") as f:
        return f.read()


def list_directory_simple(path: str) -> str:
    """
    Return a list of files in the directory and subdirectories as a
    list of absolute file paths, one path per line.
    """
    root = Path(path).absolute()
    if root != jail and jail not in root.parents:
        return f"ERROR: Path {path} must have {jail} as an ancestor"
    if not root.exists():
        return f"ERROR: Path {root} does not exist"
    if not root.is_dir():
        return f"ERROR: Path {root} is not a directory"

    result = []

    def _tree(dir_path: Path):
        for path in [x for x in dir_path.iterdir() if x.is_file()]:
            if path.name.startswith("."):
                continue
            result.append(str(path.absolute()))

        for path in [x for x in dir_path.iterdir() if x.is_dir()]:
            if path.name.startswith("."):
                continue
            if path.is_dir():
                _tree(path)

    _tree(root)
    result = sorted(result)
    return "\n".join(result)


def str_replace(path: str, old_str: str, new_str: str) -> str:
    p = Path(path).absolute()
    if p != jail and jail not in p.parents:
        return f"ERROR: Path {path} must have {jail} as an ancestor"
    if not p.exists():
        return f"ERROR: Path {path} does not exist"
    if not p.is_file():
        return f"ERROR: Path {path} is not a file"
    with open(path, "r") as f:
        content = f.read()

    occurances = content.count(old_str)
    if occurances == 0:
        return (
            "Error: No match found for replacement."
            + " Please check your text and try again."
        )
    if occurances > 1:
        return (
            f"Error: Found {occurances} matches for replacement text."
            + " Please provide more context to make a unique match."
        )

    new_content = content.replace(old_str, new_str, 1)

    with open(path, "w") as f:
        f.write(new_content)

    return "Success editing file"


def create(path: str, file_text: str) -> str:
    p = Path(path).absolute()
    if p != jail and jail not in p.parents:
        return f"ERROR: Path {path} must have {jail} as an ancestor"
    if p.exists():
        return (
            f"ERROR: Path {path} already exists; choose another file name."
        )
    with open(path, "w") as f:
        f.write(file_text)
    return "Success creating file"


tools = [
    {
        "name": "list_directory_simple",
        "description": """
            List the complete file tree for path, including subdirectories.

            The passed path MUST be a directory, not a file.

            Use this tool to discover the files in the codebase. The returned value is a list of absolute paths. Use these paths to explore the codebase.

            Here is an example tree, with a python project in a src directory:

            /Users/mike/code/pythonapp/src/mypackage/entrypoint.py
            /Users/mike/code/pythonapp/src/mypackage/main.py
            /Users/mike/code/pythonapp/LICENSE
            /Users/mike/code/pythonapp/README.md
            /Users/mike/code/pythonapp/pyproject.toml
            /Users/mike/code/pythonapp/uv.lock

            Real lists are likely to be much larger!
        """,
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Absolute path to list files",
                }
            },
            "required": ["path"],
        },
    },
    {
        "name": "read_file_path",
        "description": """
            Read a file.
        """,
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Absolute path to a file",
                }
            },
            "required": ["path"],
        },
    },
]


if args.allow_edits:
    if not is_git_working_copy_clean(Path(args.path)):
        print("To edit, git working directory must be clean:", args.path)
        exit(1)

    tools.append(
        {
            "name": "str_replace",
            "description": """
            Edit a file by specifying an exact existing string and its replacement.
        """,
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Absolute path to a file",
                    },
                    "old_str": {
                        "type": "string",
                        "description": "Exact string to replace",
                    },
                    "new_str": {
                        "type": "string",
                        "description": "Exact replacement string",
                    },
                },
                "required": ["path"],
            },
        }
    )
    tools.append(
        {
            "name": "create",
            "description": """
            Create a file, supplying its contents.

            If the file already exists, this function will fail.
            """,
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Absolute path to a file.",
                    },
                    "file_text": {
                        "type": "string",
                        "description": "Text content for new file",
                    },
                },
                "required": ["path"],
            },
        }
    )
openai_tools = [{"type": "function", "function": x} for x in tools]


def process_tool_call(tool_name: str, tool_input: Dict[str, str]) -> str:
    match tool_name:
        case "list_directory_simple":
            return list_directory_simple(tool_input["path"])
        case "read_file_path":
            return read_file_path(tool_input["path"])
        case "str_replace":
            return str_replace(
                tool_input["path"],
                tool_input["old_str"],
                tool_input["new_str"],
            )
        case "create":
            return create(
                tool_input["path"],
                tool_input["file_text"],
            )
    return f"Error: no tool with name {tool_name}"


console = Console()

if not args.task:
    extras = Prompt.ask(
        "Anything you'd like AI to think about (eg, plan how to do X)",
        console=console,
    )
    if extras:
        user_task = extras
    else:
        user_task = "Please explain this codebase"
else:
    user_task = args.task

prompt = f"""
You are a programmer exploring a codebase.

You have access to tools that tell you your working directory, list files, read files. Use these tools to explore the code in the directory.

The best way to explore the code is to list the files in the directory using the list_directory_simple tool. It will return a list of all files in the directory tree, including subdirectories.

Choose some important looking code files in the source tree and use the the read_file_path tool to read the files. Read more files if needed to understand the purpose of the program.

If the read_file_path tool fails, double check the path you passed in!

Take your time and be sure you've looked at everything you need to understand the program and answer the user's question below.

The project root directory is: {Path(args.path).resolve()}

Here's the user's question:

{user_task}
"""

console.print(Panel(Markdown(prompt), title="Prompt"))

# dedented markdown to use when formatting each tool use message
tool_use_markdown = """
{text}

Tool Used: `{tool_name}`

Tool Input:
```json
{tool_input}
```

Tool Result:
```
{tool_result}
```
"""

chat_history = []
num_turns = 0

for i in range(0, max_turns):
    num_turns += 1
    logger.debug(f"\n{'=' * 50}")

    messages = client.prepare_messages(chat_history)

    with console.status("Model is working..."):
        logger.debug("Messaging model")
        chat_response = client.chat(
            model=chat_model,
            messages=messages,
            tools=client.tools_for_model(openai_tools),
        )

    logger.debug(f"Length of chat_history: {len(chat_history)}")

    if client.has_tool_use(chat_response):
        response_text = client.get_response_text(chat_response)
        tool_name, tool_input, tool_use_id = client.get_tool_use(
            chat_response
        )
        tool_result = process_tool_call(tool_name, tool_input)

        md = Markdown(
            tool_use_markdown.format(
                text=response_text,
                tool_name=tool_name,
                tool_input=json.dumps(tool_input, indent=2),
                tool_result="\n".join(tool_result.split("\n")[:5] + ["..."]),
            )
        )
        console.print(Panel(md, title="Turn"))

        chat_history.append(
            client.format_assistant_history_message(chat_response)
        )
        chat_history.append(
            client.format_tool_result_message(
                tool_name, tool_use_id, tool_result
            )
        )
    else:
        chat_history.append(
            client.format_assistant_history_message(chat_response)
        )
        message_text = (
            client.get_response_text(chat_response)
            .replace("<think>", "`<think>`")
            .replace("</think>", "`</think>`")
        )
        md = Markdown(message_text)
        console.print(Panel(md, title="Code exploration result"))
        if args.output:
            with open(
                Path(args.output).absolute(), "w", encoding="utf-8"
            ) as f:
                f.write(message_text)
        break

logger.info("Config: max turns: %d, path: %s", max_turns, jail)
logger.info("Took %d turns", num_turns)
logger.info("Used %s model", chat_model)
